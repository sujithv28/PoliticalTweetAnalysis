{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Political Tweet Analysis\n",
    "\n",
    "## Introduction\n",
    "In this notebook we highlight Data Science and Natural Language Processing methods to analyze tweets pertaining to US Presidential Nominees: Hillary Clinton (Democratic Party) and Donald Trump (Republican Party) before and after the election. Our analysis focuses on sentiment analysis related to each party over time, network effects/flow (echo chamber), and any other election factors.\n",
    "\n",
    "Before following through this notebook please install all required packages listed in the [references.txt](references.txt) file as well as following the steps listed in the [README](README.md) to download the NLTK corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 2/3 compatibility\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import os, pwd\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper methods which tokenize, and convert the content string\n",
    "# to a list of words (can also handle #'s, @'s, etc)\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', '...', 'I']\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "def extract_http_link(s):\n",
    "    r = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(r, s)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "def timestr_to_datetime(timestr):\n",
    "    time = None\n",
    "    try:\n",
    "        time = datetime.strptime(timestr, '%m/%d/%Y %H:%M:%S')\n",
    "    except ValueError, e:\n",
    "        print('%s: %s' % (e, timestr))\n",
    "    return time\n",
    "    \n",
    "def geostamp_to_list(geostamp_str):\n",
    "    list = []\n",
    "    try:\n",
    "        if (geostamp_str != ''):\n",
    "            locations_str = geostamp_str.replace('[', '').split('],')\n",
    "            lists = [map(float, s.replace(']', '').split(',')) for s in locations_str]\n",
    "            list = lists\n",
    "    except ValueError, e:\n",
    "        print('%s: %s' % (e, geostamp_str))\n",
    "    return list\n",
    "    \n",
    "def tweet_to_list(tweet):\n",
    "    # Filter out 'RT' text if it's a retweet\n",
    "    if len(tweet) > 2 and tweet[:2] == 'RT':\n",
    "        tweet = tweet[3:]\n",
    "    lst = [term for term in preprocess(tweet) if term not in stop]\n",
    "    lst = [item.lower() for item in lst]\n",
    "    return lst\n",
    "\n",
    "def get_label(sentiment):\n",
    "    if sentiment > 0:\n",
    "        return 'positive'\n",
    "    elif sentiment < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pandas Dataframe from CSV File and do data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sujith/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:66: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "username = pwd.getpwuid(os.getuid())[0]\n",
    "# Get file from Dropbox Directory \n",
    "# If you don't have access to our Dropbox then fetch tweets using tweepy and save as a CSV file)\n",
    "file_name = '/Users/{0:s}/Dropbox/US_UK_ElectionTweets/geo_time_tweets_fixed/temp_geo.csv'.format(username)\n",
    "file = open(file_name)\n",
    "\n",
    "df = pd.read_csv(file, dtype={'Geostamp': str})\n",
    "df['Content'] = df.apply(lambda row: tweet_to_list(row['Content']), axis=1)\n",
    "df['Geostamp'] = df.apply(lambda row: geostamp_to_list(row['Geostamp']), axis=1)\n",
    "df['isHillary'] = df.apply(lambda row: bool(row['isHillary']), axis=1)\n",
    "df['Timestamp'] = df.apply(lambda row: timestr_to_datetime(row['Date'] + ' ' + row['Time']), axis=1)\n",
    "df.drop(df.columns[len(df.columns) - 1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create global variables we will use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = None\n",
    "corpus = {\n",
    "    'all': [],\n",
    "    'hillary': [],\n",
    "    'trump': [],\n",
    "    'positive': [],\n",
    "    'neutral': [],\n",
    "    'negative': []\n",
    "}\n",
    "terms = {\n",
    "    'all': [],\n",
    "    'filtered': [],\n",
    "    'hillary': [],\n",
    "    'trump': [],\n",
    "    'positive': [],\n",
    "    'neutral': [],\n",
    "    'negative': []\n",
    "}\n",
    "terms_all_counter = Counter()\n",
    "terms_filtered_counter = Counter()\n",
    "tfidf_matrix = None\n",
    "geo_data = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in variables with Twitter Data for use in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, tweet in df.iterrows():\n",
    "    # Temporary Fix\n",
    "    tweet['Content'] = [term.lower() for term in tweet['Content']]\n",
    "\n",
    "    str = ' '.join(tweet['Content'])\n",
    "    unicode_tweet = unicode(str, errors='replace')\n",
    "    corpus['all'].append(unicode_tweet)\n",
    "\n",
    "    if (tweet['isHillary']):\n",
    "        terms['hillary'].extend(tweet['Content'])\n",
    "        corpus['hillary'].append(unicode_tweet)\n",
    "    else:\n",
    "        terms['trump'].extend(tweet['Content'])\n",
    "        corpus['trump'].append(unicode_tweet)\n",
    "\n",
    "    filtered_list = [term for term in tweet['Content'] if not term.startswith(('#', '@'))]\n",
    "    terms['filtered'].extend(filtered_list)\n",
    "    terms['all'].extend(tweet['Content'])\n",
    "\n",
    "    sentiment = get_label(tweet['Compound'])\n",
    "    terms[sentiment].extend(tweet['Content'])\n",
    "    corpus[sentiment].append(unicode_tweet)\n",
    "\n",
    "    if tweet['Geostamp']:\n",
    "        time = tweet['Timestamp'].strftime('%m/%d/%Y %H:%M:%S').encode('utf-8').strip()\n",
    "        latlang = tweet['Geostamp'][0]\n",
    "        latlang[0], latlang[1] = latlang[1], latlang[0]\n",
    "        coordinates = {'coordinates': latlang, 'type': 'Point'}\n",
    "        geo_json_feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': coordinates,\n",
    "            'properties': {\n",
    "                'text': unicode_tweet,\n",
    "                'created_at': time\n",
    "            }\n",
    "        }\n",
    "        geo_data['features'].append(geo_json_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] LDA Model Topics\n",
      "[(12, u'0.265*\"\\ufffd\" + 0.036*\"#notmypresident\" + 0.018*\"union\" + 0.015*\"square\" + 0.014*\"my\" + 0.012*\"this\" + 0.011*\"i\\'m\" + 0.010*\"#nevertrump\" + 0.010*\"not\" + 0.009*\"#imstillwithher\"'), (7, u'0.051*\"#notmypresident\" + 0.025*\"#protest\" + 0.020*\"trump\" + 0.017*\"#imwithher\" + 0.014*\"hillary\" + 0.013*\"#nevertrump\" + 0.013*\"#trumpprotest\" + 0.013*\"hate\" + 0.011*\"trumps\" + 0.011*\"watch\"')]\n"
     ]
    }
   ],
   "source": [
    "texts = [[word for word in document.lower().split()] for document in corpus['all']]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(lda_corpus)\n",
    "corpus_tfidf = tfidf[lda_corpus]\n",
    "# Initialize an LDA transformation on the data\n",
    "lda = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=20)\n",
    "lda.save('tweet_lda_model.lsi')\n",
    "print(lda.print_topics(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFIDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got signatures                           0.214273833351\n",
      "got signatures number                    0.214273833351\n",
      "increasing quickly                       0.214273833351\n",
      "link petition                            0.214273833351\n",
      "link petition ve                         0.214273833351\n",
      "notmypresident tweet link                0.214273833351\n",
      "number increasing                        0.214273833351\n",
      "number increasing quickly                0.214273833351\n",
      "petition ve                              0.214273833351\n",
      "petition ve got                          0.214273833351\n",
      "signatures number                        0.214273833351\n",
      "signatures number increasing             0.214273833351\n",
      "tweet link                               0.214273833351\n",
      "tweet link petition                      0.214273833351\n",
      "ve got signatures                        0.214273833351\n",
      "notmypresident tweet                     0.205145032321\n",
      "increasing                               0.198668044914\n",
      "number                                   0.186068634808\n",
      "quickly                                  0.183062256478\n",
      "link                                     0.178038314737\n",
      "ve got                                   0.178038314737\n",
      "signatures                               0.173933455448\n",
      "petition                                 0.140919765551\n",
      "tweet                                    0.132081476856\n",
      "ve                                       0.124051156786\n",
      "got                                      0.118782933514\n",
      "notmypresident                           0.0341168661497\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')\n",
    "tfidf_matrix =  tf.fit_transform(corpus['all'])\n",
    "feature_names = tf.get_feature_names()\n",
    "dense = tfidf_matrix.todense()\n",
    "dense_tweets = dense[0].tolist()[0]\n",
    "phrase_scores = [pair for pair in zip(range(0, len(dense_tweets)), dense_tweets) if pair[1] > 0]\n",
    "sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n",
    "for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:len(sorted_phrase_scores)]:\n",
    "    print('{0: <40} {1}'.format(phrase, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity between Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Tweets Similar To: fuck @realdonaldtrump don't deserve respect never gave respect #notmypresident fake bastard https://t.co/hddodydbua\n",
      "0.19  ->  if can't handle @realdonaldtrump don't deserve @barackobama #election2016 #notmypresident\n",
      "0.17  ->  @cnn @realdonaldtrump never gave respect anyone doesn't deserve respect matter he's racist thief asshole #notmypresident\n",
      "0.17  ->  if dealing troll trump supporters block do engage don't deserve respect #trolls #notmypresident\n",
      "0.15  ->  @egg509 respect never #notmypresident\n",
      "0.10  ->  stop telling respect trump man done nothing say things made lose respect #notmypresident\n"
     ]
    }
   ],
   "source": [
    "# Helper method which finds cosine similarities given a tfidf matrix and an index of a tweet in matrix\n",
    "def find_cosine_similar(tfidf_matrix, index, top_n=5):\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[index:index + 1], tfidf_matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]\n",
    "    \n",
    "# Select a random tweet to find similar tweets based on cosine similarity\n",
    "random_tweet = corpus['all'][20]\n",
    "print('\\n[INFO] Tweets Similar To: %s' % (random_tweet))\n",
    "for index, score in find_cosine_similar(tfidf_matrix, 20):\n",
    "    print('%.2f  ->  %s' % (score, corpus['all'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump Geodata into JSON File to visualize\n",
    "To visualize the data follow the steps listed in the [README](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('geo_data.json', 'w') as fout:\n",
    "    print('\\n[INFO] Dumped geo data into geo_data.json')\n",
    "    fout.write(json.dumps(self.geo_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Counters to see most popular terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print out 10 most frequent words filtered\n",
    "terms_filtered_counter = Counter(terms['filtered'])\n",
    "for word, count in terms_filtered_counter.most_common(15):\n",
    "    print('{0}: {1}'.format(word, count))\n",
    "\n",
    "# Print out 10 most unfiltered frequent words\n",
    "terms_all_counter = Counter(terms['all'])\n",
    "for word, count in terms_all_counter.most_common(15):\n",
    "    print('{0}: {1}'.format(word, count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
