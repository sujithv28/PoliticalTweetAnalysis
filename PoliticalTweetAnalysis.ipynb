{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Political Tweet Analysis\n",
    "\n",
    "## Introduction\n",
    "In this notebook we highlight Data Science and Natural Language Processing methods to analyze tweets pertaining to US Presidential Nominees: Hillary Clinton (Democratic Party) and Donald Trump (Republican Party) before and after the election. Our analysis focuses on sentiment analysis related to each party over time, network effects/flow (echo chamber), and any other election factors.\n",
    "\n",
    "Before following through this notebook please install all required packages listed in the [references.txt](references.txt) file as well as following the steps listed in the [README](README.md) to download the NLTK corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 2/3 compatibility\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import os, pwd\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper methods which tokenize, and convert the content string\n",
    "# to a list of words (can also handle #'s, @'s, etc)\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', '...', 'I']\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "def extract_http_link(s):\n",
    "    r = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(r, s)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "def timestr_to_datetime(timestr):\n",
    "    time = None\n",
    "    try:\n",
    "        time = datetime.strptime(timestr, '%m/%d/%Y %H:%M:%S')\n",
    "    except ValueError, e:\n",
    "        print('%s: %s' % (e, timestr))\n",
    "    return time\n",
    "    \n",
    "def geostamp_to_list(geostamp_str):\n",
    "    list = []\n",
    "    try:\n",
    "        if (geostamp_str != ''):\n",
    "            locations_str = geostamp_str.replace('[', '').split('],')\n",
    "            lists = [map(float, s.replace(']', '').split(',')) for s in locations_str]\n",
    "            list = lists\n",
    "    except ValueError, e:\n",
    "        print('%s: %s' % (e, geostamp_str))\n",
    "    return list\n",
    "    \n",
    "def tweet_to_list(tweet):\n",
    "    # Filter out 'RT' text if it's a retweet\n",
    "    if len(tweet) > 2 and tweet[:2] == 'RT':\n",
    "        tweet = tweet[3:]\n",
    "    lst = [term for term in preprocess(tweet) if term not in stop]\n",
    "    lst = [item.lower() for item in lst]\n",
    "    return lst\n",
    "\n",
    "def get_label(sentiment):\n",
    "    if sentiment > 0:\n",
    "        return 'positive'\n",
    "    elif sentiment < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pandas Dataframe from CSV File and do data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "username = pwd.getpwuid(os.getuid())[0]\n",
    "# Get file from Dropbox Directory \n",
    "# If you don't have access to our Dropbox then fetch tweets using tweepy and save as a CSV file)\n",
    "file_name = '/Users/{0:s}/Dropbox/US_UK_ElectionTweets/geo_time_tweets_fixed/temp_geo.csv'.format(username)\n",
    "file = open(file_name)\n",
    "\n",
    "df = pd.read_csv(file, dtype={'Geostamp': str})\n",
    "df['Content'] = df.apply(lambda row: tweet_to_list(row['Content']), axis=1)\n",
    "df['Geostamp'] = df.apply(lambda row: geostamp_to_list(row['Geostamp']), axis=1)\n",
    "df['isHillary'] = df.apply(lambda row: bool(row['isHillary']), axis=1)\n",
    "df['Timestamp'] = df.apply(lambda row: timestr_to_datetime(row['Date'] + ' ' + row['Time']), axis=1)\n",
    "df.drop(df.columns[len(df.columns) - 1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create global variables we will use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = None\n",
    "corpus = {\n",
    "    'all': [],\n",
    "    'hillary': [],\n",
    "    'trump': [],\n",
    "    'positive': [],\n",
    "    'neutral': [],\n",
    "    'negative': []\n",
    "}\n",
    "terms = {\n",
    "    'all': [],\n",
    "    'filtered': [],\n",
    "    'hillary': [],\n",
    "    'trump': [],\n",
    "    'positive': [],\n",
    "    'neutral': [],\n",
    "    'negative': []\n",
    "}\n",
    "terms_all_counter = Counter()\n",
    "terms_filtered_counter = Counter()\n",
    "tfidf_matrix = None\n",
    "geo_data = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in variables with Twitter Data for use in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, tweet in df.iterrows():\n",
    "    # Temporary Fix\n",
    "    tweet['Content'] = [term.lower() for term in tweet['Content']]\n",
    "\n",
    "    str = ' '.join(tweet['Content'])\n",
    "    unicode_tweet = unicode(str, errors='replace')\n",
    "    corpus['all'].append(unicode_tweet)\n",
    "\n",
    "    if (tweet['isHillary']):\n",
    "        terms['hillary'].extend(tweet['Content'])\n",
    "        corpus['hillary'].append(unicode_tweet)\n",
    "    else:\n",
    "        terms['trump'].extend(tweet['Content'])\n",
    "        corpus['trump'].append(unicode_tweet)\n",
    "\n",
    "    filtered_list = [term for term in tweet['Content'] if not term.startswith(('#', '@'))]\n",
    "    terms['filtered'].extend(filtered_list)\n",
    "    terms['all'].extend(tweet['Content'])\n",
    "\n",
    "    sentiment = get_label(tweet['Compound'])\n",
    "    terms[sentiment].extend(tweet['Content'])\n",
    "    corpus[sentiment].append(unicode_tweet)\n",
    "\n",
    "    if tweet['Geostamp']:\n",
    "        time = tweet['Timestamp'].strftime('%m/%d/%Y %H:%M:%S').encode('utf-8').strip()\n",
    "        latlang = tweet['Geostamp'][0]\n",
    "        latlang[0], latlang[1] = latlang[1], latlang[0]\n",
    "        coordinates = {'coordinates': latlang, 'type': 'Point'}\n",
    "        geo_json_feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': coordinates,\n",
    "            'properties': {\n",
    "                'text': unicode_tweet,\n",
    "                'created_at': time\n",
    "            }\n",
    "        }\n",
    "        geo_data['features'].append(geo_json_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split()] for document in corpus['all']]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(lda_corpus)\n",
    "corpus_tfidf = tfidf[lda_corpus]\n",
    "# Initialize an LDA transformation on the data\n",
    "lda = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=20)\n",
    "lda.save('tweet_lda_model.lsi')\n",
    "print(lda.print_topics(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFIDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')\n",
    "tfidf_matrix =  tf.fit_transform(corpus['all'])\n",
    "feature_names = tf.get_feature_names()\n",
    "dense = tfidf_matrix.todense()\n",
    "dense_tweets = dense[0].tolist()[0]\n",
    "phrase_scores = [pair for pair in zip(range(0, len(dense_tweets)), dense_tweets) if pair[1] > 0]\n",
    "sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n",
    "for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:len(sorted_phrase_scores)]:\n",
    "    print('{0: <40} {1}'.format(phrase, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity between Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper method which finds cosine similarities given a tfidf matrix and an index of a tweet in matrix\n",
    "def find_cosine_similar(tfidf_matrix, index, top_n=5):\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[index:index + 1], tfidf_matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]\n",
    "    \n",
    "# Select a random tweet to find similar tweets based on cosine similarity\n",
    "random_tweet = corpus['all'][20]\n",
    "print('\\n[INFO] Tweets Similar To: %s' % (random_tweet))\n",
    "for index, score in find_cosine_similar(tfidf_matrix, 20):\n",
    "    print('%.2f  ->  %s' % (score, corpus['all'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump Geodata into JSON File to visualize\n",
    "To visualize the data follow the steps listed in the [README](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('geo_data.json', 'w') as fout:\n",
    "    print('\\n[INFO] Dumped geo data into geo_data.json')\n",
    "    fout.write(json.dumps(self.geo_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Counters to see most popular terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out 15 most frequent words filtered (no hashtags or mentions)\n",
    "print('\\n[INFO] Filtered Frequency:')\n",
    "terms_filtered_counter = Counter(terms['filtered'])\n",
    "for word, count in terms_filtered_counter.most_common(15):\n",
    "    print('{0}: {1}'.format(word, count))\n",
    "\n",
    "# Print out 15 most unfiltered frequent words\n",
    "print('\\n[INFO] Non-Filtered Frequency:')\n",
    "terms_all_counter = Counter(terms['all'])\n",
    "for word, count in terms_all_counter.most_common(15):\n",
    "    print('{0}: {1}'.format(word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
